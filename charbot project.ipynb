{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c345ac-3adb-4a82-a096-8644e43410c8",
   "metadata": {},
   "source": [
    "# Discossional Chatbot using pre-trained model\n",
    "We will use a model from the Hugging Face library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3917dd6-c73b-477a-b607-27868fe44471",
   "metadata": {},
   "source": [
    "# setup our environment\n",
    "\n",
    "### Setup python environment\n",
    "1. Download Anaconda through this link https://www.anaconda.com/download and install\n",
    "2. Once Anaconda is installed, luanch Jupyter notebook and copy this command \"!pip install transformers torch tkinter\" to install the libraries\n",
    "3. the required libraries are transformers, tourch and tkinter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bdfe5bb-1ebc-4fa8-8c6c-90508b6d16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from transformers.utils import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc24ebf-b7f7-4171-8593-2cad500109b7",
   "metadata": {},
   "source": [
    "## Download the selected pre-trained model and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1665281a-423d-47fa-80ed-2699a737eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.get_logger(\"transformers\").setLevel(logging.ERROR)\n",
    "# Load the model and tokenizer\n",
    "# model_name = \"microsoft/DialoGPT-medium\"\n",
    "model_name = \"microsoft/DialoGPT-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0f764-50ba-41f2-a1c9-449598adee31",
   "metadata": {},
   "source": [
    "## Test run the model with limit of 5 transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0931ac50-d63c-402d-8959-e6e1a16d07af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> You: how are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: I'm good, you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> You: what is the weather today\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Cloudy and raining.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> You: oh is it heavy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: It's raining.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> You: oh how much rain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: It's raining.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">> You: well its ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: It's raining again.\n"
     ]
    }
   ],
   "source": [
    "# testing the pre_trained model by chatting 5 times with the chatbot\n",
    "for step in range(5):\n",
    "    # take user input\n",
    "    text = input(\">> You:\")\n",
    "    # encode the input and add end of string token\n",
    "    input_val = tokenizer.encode(text + tokenizer.eos_token, return_tensors=\"pt\")\n",
    "    # concatenate new user input with chat history (if there is)\n",
    "    bot_input_val = torch.cat([chat_history_val, input_val], dim=-1) if step > 0 else input_val\n",
    "    # generate a bot response\n",
    "    chat_history_val = model.generate(\n",
    "        bot_input_val,\n",
    "        max_length=1000,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    #print the output\n",
    "    output = tokenizer.decode(chat_history_val[:, bot_input_val.shape[-1]:][0], skip_special_tokens=True)\n",
    "    print(f\"Chatbot: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b7a4e-81ec-4f52-be2d-bfa5cc2e5e17",
   "metadata": {},
   "source": [
    "### Putting all together in a user interface using tkinter library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e07de0d2-aab6-4b99-859a-2fd4be36d11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from transformers.utils import logging\n",
    "\n",
    "class ChatbotApp:\n",
    "    def __init__(self, root):\n",
    "        # create the user interface frame and add the components\n",
    "        self.root = root\n",
    "        self.root.title(\"Chatbot\")\n",
    "\n",
    "        self.chat_window = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=50, height=20)\n",
    "        self.chat_window.grid(column=0, row=0, padx=10, pady=10, columnspan=2)\n",
    "        self.chat_window.configure(state='disabled')\n",
    "\n",
    "        self.entry_label = tk.Label(root, text=\"Ask your question:\")\n",
    "        self.entry_label.grid(column=0, row=1, padx=10, pady=10)\n",
    "\n",
    "        self.entry_text = tk.Entry(root, width=40)\n",
    "        self.entry_text.grid(column=0, row=2, padx=10, pady=10)\n",
    "\n",
    "        self.ask_button = tk.Button(root, text=\"Ask\", command=self.ask_question)\n",
    "        self.ask_button.grid(column=1, row=2, padx=10, pady=10)\n",
    "\n",
    "        # Load the model and tokenizer\n",
    "        logging.get_logger(\"transformers\").setLevel(logging.ERROR)\n",
    "        model_name = \"microsoft/DialoGPT-large\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        self.step = 0\n",
    "        \n",
    "    # Get chatbot response to questions\n",
    "    def get_response(self, text):\n",
    "        # encode the input and add end of string token\n",
    "        input_val = self.tokenizer.encode(text + self.tokenizer.eos_token, return_tensors=\"pt\")\n",
    "        # concatenate new user input with chat history (if there is)\n",
    "        bot_input_val = torch.cat([self.chat_history_val, input_val], dim=-1) if self.step > 0 else input_val\n",
    "        # generate a bot response\n",
    "        self.chat_history_val = self.model.generate(\n",
    "            bot_input_val,\n",
    "            max_length=1000,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        # get the response to the output\n",
    "        output = self.tokenizer.decode(self.chat_history_val[:, bot_input_val.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "        self.step +=1\n",
    "        return f\"{output}\"\n",
    "\n",
    "    def ask_question(self):\n",
    "        question = self.entry_text.get()\n",
    "        if question:\n",
    "            self.chat_window.configure(state='normal')\n",
    "            self.chat_window.insert(tk.END, \"You: \" + question + \"\\n\")\n",
    "            self.chat_window.configure(state='disabled')\n",
    "            self.entry_text.delete(0, tk.END)\n",
    "            \n",
    "            response = self.get_response(question)\n",
    "            self.chat_window.configure(state='normal')\n",
    "            self.chat_window.insert(tk.END, \"Bot: \" + response + \"\\n\\n\")\n",
    "            self.chat_window.configure(state='disabled')\n",
    "            self.chat_window.yview(tk.END)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ChatbotApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e03db-5cae-4676-ac67-a9cd8ac7eac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
