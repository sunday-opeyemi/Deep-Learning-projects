{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0079dafa-fd13-48ad-8ad2-c18587ba8f89",
   "metadata": {},
   "source": [
    "### Project title\n",
    "# Development of a Bimodal Biometric Crime Control System using a Modified Neura Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08a7d4-61bf-44c8-b10c-f28c0fa01ab6",
   "metadata": {},
   "source": [
    "Bimodal biometric systems represent a strategic evolution in crime control technologies. By leveraging the complementary strengths of face and fingerprint recognition, and harnessing the power of deep learning, these systems offer a robust, reliable, and efficient solution for identity verification in high-stakes security environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d1117-7462-4a5a-bc88-c72c217d2fed",
   "metadata": {},
   "source": [
    "# Training Strategy\n",
    "### Dataset Preparation\n",
    "Biometric Databases: Use comprehensive datasets that include both modalities, ensuring they represent real-world variability.\n",
    "\n",
    "### Data Augmentation\n",
    "Augment the training set to simulate variations such as different angles, lighting conditions, and partial occlusions. Techniques may include rotation, scaling, and translation.\n",
    "Labeling and Ground Truth: Ensure that each sample is correctly labeled to supervise training.\n",
    "\n",
    "### Training Process\n",
    "Loss Functions: Use cross-entropy loss for classification tasks, potentially combined with additional regularization terms to prevent overfitting.\n",
    "Optimization: Experiment with optimizers like Adam or SGD with momentum. Fine-tune learning rates and implement learning rate decay strategies.\n",
    "Validation: Use a dedicated validation set to monitor model performance and prevent overfitting.\n",
    "\n",
    "### Evaluation Metrics and Testing\n",
    "Accuracy: The overall rate of correct identifications.\n",
    "Receiver Operating Characteristic (ROC) Curve: Used to evaluate the trade-off between true positive rate and false positive rate.\n",
    "Confusion Matrix: Provides detailed insights into classification errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d937f0ff-8f1f-4f6d-b9df-1d7c5a1b3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c49fed-cd08-4081-8ae8-54554e84aede",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "Load and preprocess BMP images from the given folder.\n",
    "Images are read in grayscale, resized, and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecf40ab3-eae8-46ed-b314-e631b9a7b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, resize_shape=(128, 128)):\n",
    "    images = {}\n",
    "    # Get all BMP files (case-insensitive)\n",
    "    file_paths = glob(os.path.join(folder, \"*.BMP\"))\n",
    "    for file_path in file_paths:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            continue  # Skip if image cannot be read\n",
    "        img = cv2.resize(img, resize_shape)\n",
    "        img = img / 255.0  # Normalize pixel values to [0, 1]\n",
    "        images[file_name] = img\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfb3002-55f5-4fbc-b989-2627c5834ac4",
   "metadata": {},
   "source": [
    "### Organize and match face and fingerprint images.\n",
    "\n",
    "Iterates through each subject folder in base_path (e.g., '1' to '45'),\n",
    "loads fingerprint, left face, and right face images, and creates samples.\n",
    "Each sample consists of one fingerprint image paired with a representative\n",
    "left and right face image for that subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "648aaff3-03df-4bf9-b863-9ed3b675333b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples processed: 450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '1',\n",
       "  'fingerprint': array([[0.62745098, 0.62745098, 0.62745098, ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.62745098, 0.53333333, 0.4745098 , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.62745098, 0.49411765, 0.5254902 , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ]]),\n",
       "  'left_face': array([[0.96862745, 0.9372549 , 0.95294118, ..., 0.59607843, 0.58039216,\n",
       "          0.55294118],\n",
       "         [0.91372549, 0.95686275, 0.9254902 , ..., 0.60392157, 0.58823529,\n",
       "          0.55294118],\n",
       "         [0.93333333, 0.97254902, 0.97254902, ..., 0.59607843, 0.59607843,\n",
       "          0.57647059],\n",
       "         ...,\n",
       "         [0.83529412, 0.85490196, 0.89803922, ..., 0.62745098, 0.61568627,\n",
       "          0.60784314],\n",
       "         [0.84705882, 0.87843137, 0.8745098 , ..., 0.61960784, 0.61176471,\n",
       "          0.59607843],\n",
       "         [0.43921569, 0.44313725, 0.43529412, ..., 0.35686275, 0.34901961,\n",
       "          0.34509804]]),\n",
       "  'right_face': array([[0.41176471, 0.38039216, 0.39607843, ..., 0.76470588, 0.54901961,\n",
       "          0.74901961],\n",
       "         [0.39607843, 0.39215686, 0.38823529, ..., 0.70588235, 0.61568627,\n",
       "          0.73333333],\n",
       "         [0.39215686, 0.38431373, 0.37647059, ..., 0.83137255, 0.63921569,\n",
       "          0.77647059],\n",
       "         ...,\n",
       "         [0.60784314, 0.60392157, 0.60784314, ..., 0.65490196, 0.63921569,\n",
       "          0.63529412],\n",
       "         [0.59607843, 0.61176471, 0.60784314, ..., 0.65490196, 0.63921569,\n",
       "          0.62352941],\n",
       "         [0.35294118, 0.35686275, 0.36470588, ..., 0.36862745, 0.37647059,\n",
       "          0.36078431]]),\n",
       "  'fp_name': '1__M_Left_index_finger.BMP'},\n",
       " {'id': '1',\n",
       "  'fingerprint': array([[0.62745098, 0.62745098, 0.62745098, ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.62745098, 0.53333333, 0.4745098 , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.62745098, 0.49411765, 0.5254902 , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ]]),\n",
       "  'left_face': array([[0.96862745, 0.9372549 , 0.95294118, ..., 0.59607843, 0.58039216,\n",
       "          0.55294118],\n",
       "         [0.91372549, 0.95686275, 0.9254902 , ..., 0.60392157, 0.58823529,\n",
       "          0.55294118],\n",
       "         [0.93333333, 0.97254902, 0.97254902, ..., 0.59607843, 0.59607843,\n",
       "          0.57647059],\n",
       "         ...,\n",
       "         [0.83529412, 0.85490196, 0.89803922, ..., 0.62745098, 0.61568627,\n",
       "          0.60784314],\n",
       "         [0.84705882, 0.87843137, 0.8745098 , ..., 0.61960784, 0.61176471,\n",
       "          0.59607843],\n",
       "         [0.43921569, 0.44313725, 0.43529412, ..., 0.35686275, 0.34901961,\n",
       "          0.34509804]]),\n",
       "  'right_face': array([[0.41176471, 0.38039216, 0.39607843, ..., 0.76470588, 0.54901961,\n",
       "          0.74901961],\n",
       "         [0.39607843, 0.39215686, 0.38823529, ..., 0.70588235, 0.61568627,\n",
       "          0.73333333],\n",
       "         [0.39215686, 0.38431373, 0.37647059, ..., 0.83137255, 0.63921569,\n",
       "          0.77647059],\n",
       "         ...,\n",
       "         [0.60784314, 0.60392157, 0.60784314, ..., 0.65490196, 0.63921569,\n",
       "          0.63529412],\n",
       "         [0.59607843, 0.61176471, 0.60784314, ..., 0.65490196, 0.63921569,\n",
       "          0.62352941],\n",
       "         [0.35294118, 0.35686275, 0.36470588, ..., 0.36862745, 0.37647059,\n",
       "          0.36078431]]),\n",
       "  'fp_name': '1__M_Left_little_finger.BMP'},\n",
       " {'id': '1',\n",
       "  'fingerprint': array([[0.62745098, 0.62745098, 0.62745098, ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.62745098, 0.53333333, 0.4745098 , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.62745098, 0.49411765, 0.5254902 , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "          0.        ]]),\n",
       "  'left_face': array([[0.96862745, 0.9372549 , 0.95294118, ..., 0.59607843, 0.58039216,\n",
       "          0.55294118],\n",
       "         [0.91372549, 0.95686275, 0.9254902 , ..., 0.60392157, 0.58823529,\n",
       "          0.55294118],\n",
       "         [0.93333333, 0.97254902, 0.97254902, ..., 0.59607843, 0.59607843,\n",
       "          0.57647059],\n",
       "         ...,\n",
       "         [0.83529412, 0.85490196, 0.89803922, ..., 0.62745098, 0.61568627,\n",
       "          0.60784314],\n",
       "         [0.84705882, 0.87843137, 0.8745098 , ..., 0.61960784, 0.61176471,\n",
       "          0.59607843],\n",
       "         [0.43921569, 0.44313725, 0.43529412, ..., 0.35686275, 0.34901961,\n",
       "          0.34509804]]),\n",
       "  'right_face': array([[0.41176471, 0.38039216, 0.39607843, ..., 0.76470588, 0.54901961,\n",
       "          0.74901961],\n",
       "         [0.39607843, 0.39215686, 0.38823529, ..., 0.70588235, 0.61568627,\n",
       "          0.73333333],\n",
       "         [0.39215686, 0.38431373, 0.37647059, ..., 0.83137255, 0.63921569,\n",
       "          0.77647059],\n",
       "         ...,\n",
       "         [0.60784314, 0.60392157, 0.60784314, ..., 0.65490196, 0.63921569,\n",
       "          0.63529412],\n",
       "         [0.59607843, 0.61176471, 0.60784314, ..., 0.65490196, 0.63921569,\n",
       "          0.62352941],\n",
       "         [0.35294118, 0.35686275, 0.36470588, ..., 0.36862745, 0.37647059,\n",
       "          0.36078431]]),\n",
       "  'fp_name': '1__M_Left_middle_finger.BMP'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def structure_dataset(base_path):\n",
    "    dataset = []\n",
    "    \n",
    "    # Get a sorted list of all subject folders in the base path\n",
    "    subject_folders = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
    "    \n",
    "    for subject in subject_folders:\n",
    "        subject_path = os.path.join(base_path, subject)\n",
    "        fingerprint_folder = os.path.join(subject_path, \"Fingerprint\")\n",
    "        left_folder = os.path.join(subject_path, \"left\")\n",
    "        right_folder = os.path.join(subject_path, \"right\")\n",
    "        \n",
    "        # Load images from each modality\n",
    "        fingerprints = load_images_from_folder(fingerprint_folder)\n",
    "        left_faces = load_images_from_folder(left_folder)\n",
    "        right_faces = load_images_from_folder(right_folder)\n",
    "        \n",
    "        # Skip subject if any modality is missing\n",
    "        if not fingerprints or not left_faces or not right_faces:\n",
    "            print(f\"Skipping subject {subject}: Missing modality\")\n",
    "            continue\n",
    "        \n",
    "        # Choose a representative left and right face image (first sorted image)\n",
    "        left_face_img = left_faces[sorted(left_faces.keys())[0]]\n",
    "        right_face_img = right_faces[sorted(right_faces.keys())[0]]\n",
    "        \n",
    "        # For each fingerprint image, create a sample with the corresponding face images\n",
    "        for fp_name, fp_img in fingerprints.items():\n",
    "            sample = {\n",
    "                \"id\": subject,\n",
    "                \"fingerprint\": fp_img,\n",
    "                \"left_face\": left_face_img,\n",
    "                \"right_face\": right_face_img,\n",
    "                \"fp_name\": fp_name\n",
    "            }\n",
    "            dataset.append(sample)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Set the base path to the folder containing the dataset\n",
    "base_path = \"Project Assignments/Tumininu Akibowale/IRIS and FINGERPRINT DATASET\"\n",
    "\n",
    "dataset = structure_dataset(base_path)\n",
    "print(\"Total samples processed:\", len(dataset))\n",
    "dataset[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e91f71-aefc-495f-ba5e-bbfe0ac77ce4",
   "metadata": {},
   "source": [
    "# Data Splitting and Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b73f55-df15-4e28-8ef0-197699a20e47",
   "metadata": {},
   "source": [
    "We create a BimodalBiometricDataset class that converts the preprocessed NumPy arrays (for fingerprint, left face, and right face) into PIL images, applies transforms (including data augmentation for training), and assigns a numerical label to each subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eebb6632-3529-45dd-bd23-5e3775f85a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class BimodalBiometricDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        # Create a mapping from subject id to a numeric label.\n",
    "        self.subjects = sorted(list(set(item[\"id\"] for item in data)))\n",
    "        self.subject2label = {subject: idx for idx, subject in enumerate(self.subjects)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        # Each modality is stored as a NumPy array in the range [0, 1]\n",
    "        fingerprint = sample[\"fingerprint\"]\n",
    "        left_face = sample[\"left_face\"]\n",
    "        right_face = sample[\"right_face\"]\n",
    "\n",
    "        # Convert numpy arrays to PIL images (we assumes images are grayscale)\n",
    "        fingerprint_img = Image.fromarray((fingerprint * 255).astype(np.uint8), mode='L')\n",
    "        left_face_img = Image.fromarray((left_face * 255).astype(np.uint8), mode='L')\n",
    "        right_face_img = Image.fromarray((right_face * 255).astype(np.uint8), mode='L')\n",
    "\n",
    "        # Apply the transform (data augmentation)\n",
    "        if self.transform:\n",
    "            fingerprint_img = self.transform(fingerprint_img)\n",
    "            left_face_img = self.transform(left_face_img)\n",
    "            right_face_img = self.transform(right_face_img)\n",
    "        else:\n",
    "            # Default conversion to tensor if no transform is provided\n",
    "            fingerprint_img = transforms.ToTensor()(fingerprint_img)\n",
    "            left_face_img = transforms.ToTensor()(left_face_img)\n",
    "            right_face_img = transforms.ToTensor()(right_face_img)\n",
    "\n",
    "        # Get the numeric label for the subject\n",
    "        label = self.subject2label[sample[\"id\"]]\n",
    "        return {\"fingerprint\": fingerprint_img, \n",
    "                \"left_face\": left_face_img, \n",
    "                \"right_face\": right_face_img, \n",
    "                \"label\": label}\n",
    "\n",
    "# Define transforms\n",
    "# For training: include data augmentation (random horizontal flip, rotation) and conversion to tensor.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa4cf4-dbd9-4a91-b7d3-d1e168aedeff",
   "metadata": {},
   "source": [
    "# Split the Data: \n",
    "Divide your dataset into training, validation, and testing sets (common splits are 70% for training, 15% for validation, and 15% for testing). This ensures you can both train your model and evaluate its performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "329092f2-d20f-4bf1-a3db-3faaa16e1d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 450\n",
      "Train samples: 315, Validation samples: 67, Test samples: 68\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset object using the train transforms initially.\n",
    "dataset_obj = BimodalBiometricDataset(dataset, transform=train_transforms)\n",
    "\n",
    "# Data splitting: 70% train, 15% validation, 15% test.\n",
    "total_samples = len(dataset_obj)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.15 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_obj, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Train samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c64f7f-a6b4-40ab-9bb8-2c74c4806532",
   "metadata": {},
   "source": [
    "# Data Augmentation: \n",
    "Apply augmentation techniques (e.g., rotations, translations, scaling) to the training data. This is particularly important because our dataset is limited, as it will help improve the model's robustness and prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2c08e51-908a-46a0-a78d-3246d060f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fingerprint batch shape: torch.Size([8, 1, 128, 128])\n",
      "Left face batch shape: torch.Size([8, 1, 128, 128])\n",
      "Right face batch shape: torch.Size([8, 1, 128, 128])\n",
      "Labels: tensor([19, 33,  5,  8, 13, 12, 40,  4])\n"
     ]
    }
   ],
   "source": [
    "# For validation/test: simply convert to tensor.\n",
    "test_transforms = transforms.ToTensor()\n",
    "\n",
    "# For validation and test sets, override the transform to remove augmentation.\n",
    "# Create new dataset instances for validation and test with test_transforms.\n",
    "# (Since random_split returns Subset objects, we can manually set their dataset.transform.)\n",
    "train_dataset.dataset.transform = train_transforms  # training with augmentation\n",
    "val_dataset.dataset.transform = test_transforms      # no augmentation for validation\n",
    "test_dataset.dataset.transform = test_transforms     # no augmentation for testing\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Test one batch to verify the outputs\n",
    "for batch in train_loader:\n",
    "    print(\"Fingerprint batch shape:\", batch['fingerprint'].shape)\n",
    "    print(\"Left face batch shape:\", batch['left_face'].shape)\n",
    "    print(\"Right face batch shape:\", batch['right_face'].shape)\n",
    "    print(\"Labels:\", batch['label'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801d8e2-6cac-43ac-aa90-dd87b11e7319",
   "metadata": {},
   "source": [
    "# Data Modelling\n",
    "\n",
    "We will create neural network that processes three modalities (left face, right face, and fingerprint), fuses their features, and produces a classification output. Each branch uses a simple convolutional neural network (CNN) that extracts a 128-dimensional feature vector. These three feature vectors are then concatenated into a 384-dimensional vector, which is passed through fully connected layers to generate the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad4892ef-c675-4d92-8c2b-9cddfdac7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CNN for processing face images (used for both left and right views)\n",
    "class FaceCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, 1, 128, 128)\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> (batch, 32, 64, 64)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> (batch, 64, 32, 32)\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # -> (batch, 128, 16, 16)\n",
    "        # Use adaptive average pooling to reduce spatial dimensions to 1x1\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))  # -> (batch, 128, 1, 1)\n",
    "        x = x.view(x.size(0), -1)             # -> (batch, 128)\n",
    "        return x\n",
    "\n",
    "# CNN for processing fingerprint images (similar structure)\n",
    "class FingerprintCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FingerprintCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch, 1, 128, 128)\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> (batch, 32, 64, 64)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> (batch, 64, 32, 32)\n",
    "        x = self.pool(F.relu(self.conv3(x)))  # -> (batch, 128, 16, 16)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))    # -> (batch, 128, 1, 1)\n",
    "        x = x.view(x.size(0), -1)               # -> (batch, 128)\n",
    "        return x\n",
    "\n",
    "# Combined network that fuses features from both face branches and the fingerprint branch.\n",
    "class BiometricNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BiometricNet, self).__init__()\n",
    "        # Instantiate separate CNNs for left and right face images.\n",
    "        self.left_face_cnn = FaceCNN()\n",
    "        self.right_face_cnn = FaceCNN()\n",
    "        # Instantiate a CNN for fingerprint images.\n",
    "        self.fingerprint_cnn = FingerprintCNN()\n",
    "        # The feature dimension from each branch is 128. Combined, we have 128 * 3 = 384.\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(384, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, left_face, right_face, fingerprint):\n",
    "        # Process each modality\n",
    "        left_feat = self.left_face_cnn(left_face)       # (batch, 128)\n",
    "        right_feat = self.right_face_cnn(right_face)      # (batch, 128)\n",
    "        fp_feat = self.fingerprint_cnn(fingerprint)       # (batch, 128)\n",
    "        # Concatenate the features from all three branches\n",
    "        combined = torch.cat((left_feat, right_feat, fp_feat), dim=1)  # (batch, 384)\n",
    "        out = self.fc(combined)  # (batch, num_classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e11f2ebb-6dd0-4e69-8d4b-7ef54837c433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:05<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 3.8200, Accuracy = 0.95%\n",
      "Epoch 1: Val Loss = 3.8146, Accuracy = 2.99%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 3.8038, Accuracy = 2.22%\n",
      "Epoch 2: Val Loss = 3.8315, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 3.7991, Accuracy = 2.86%\n",
      "Epoch 3: Val Loss = 3.8484, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 3.7961, Accuracy = 4.44%\n",
      "Epoch 4: Val Loss = 3.8878, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 3.7898, Accuracy = 3.17%\n",
      "Epoch 5: Val Loss = 3.9250, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 3.7823, Accuracy = 3.17%\n",
      "Epoch 6: Val Loss = 4.0449, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 3.7752, Accuracy = 3.81%\n",
      "Epoch 7: Val Loss = 4.0284, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 3.7703, Accuracy = 4.76%\n",
      "Epoch 8: Val Loss = 4.0632, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 3.7172, Accuracy = 4.76%\n",
      "Epoch 9: Val Loss = 3.9225, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 3.6588, Accuracy = 6.67%\n",
      "Epoch 10: Val Loss = 3.9072, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 3.6138, Accuracy = 6.03%\n",
      "Epoch 11: Val Loss = 3.8462, Accuracy = 4.48%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 3.6050, Accuracy = 8.25%\n",
      "Epoch 12: Val Loss = 3.9389, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 3.5138, Accuracy = 7.94%\n",
      "Epoch 13: Val Loss = 3.8352, Accuracy = 2.99%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 3.4235, Accuracy = 10.79%\n",
      "Epoch 14: Val Loss = 3.8060, Accuracy = 8.96%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 3.3804, Accuracy = 9.84%\n",
      "Epoch 15: Val Loss = 3.6745, Accuracy = 10.45%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 3.3223, Accuracy = 12.70%\n",
      "Epoch 16: Val Loss = 3.6038, Accuracy = 5.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 3.0977, Accuracy = 18.41%\n",
      "Epoch 17: Val Loss = 3.3771, Accuracy = 19.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 3.0684, Accuracy = 15.56%\n",
      "Epoch 18: Val Loss = 3.2381, Accuracy = 20.90%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss = 2.8333, Accuracy = 20.32%\n",
      "Epoch 19: Val Loss = 2.9942, Accuracy = 19.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss = 2.6692, Accuracy = 21.27%\n",
      "Epoch 20: Val Loss = 2.9438, Accuracy = 16.42%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration: use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Number of classes (subjects)\n",
    "num_classes = 45\n",
    "\n",
    "# Instantiate the model and move it to the device\n",
    "model = BiometricNet(num_classes=num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_train = 0\n",
    "    correct_train = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # Get data and move to device\n",
    "        left_face = batch['left_face'].to(device)\n",
    "        right_face = batch['right_face'].to(device)\n",
    "        fingerprint = batch['fingerprint'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(left_face, right_face, fingerprint)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss and correct predictions\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "        total_train += labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_train += torch.sum(preds == labels).item()\n",
    "    \n",
    "    epoch_loss = running_loss / total_train\n",
    "    epoch_acc = (correct_train / total_train) * 100\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    total_val = 0\n",
    "    correct_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            left_face = batch['left_face'].to(device)\n",
    "            right_face = batch['right_face'].to(device)\n",
    "            fingerprint = batch['fingerprint'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(left_face, right_face, fingerprint)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_val_loss += loss.item() * labels.size(0)\n",
    "            total_val += labels.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_val += torch.sum(preds == labels).item()\n",
    "    \n",
    "    val_loss = running_val_loss / total_val\n",
    "    val_acc = (correct_val / total_val) * 100\n",
    "    print(f\"Epoch {epoch+1}: Val Loss = {val_loss:.4f}, Accuracy = {val_acc:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfc4f1-3b08-4df9-8f9e-e5a73c0d66e6",
   "metadata": {},
   "source": [
    "# Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ca9e0a6-71e0-4f83-a52d-edf4fc9d05b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss = 2.8926, Test Accuracy = 16.18%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set after training\n",
    "model.eval()\n",
    "running_test_loss = 0.0\n",
    "total_test = 0\n",
    "correct_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        left_face = batch['left_face'].to(device)\n",
    "        right_face = batch['right_face'].to(device)\n",
    "        fingerprint = batch['fingerprint'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        outputs = model(left_face, right_face, fingerprint)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        running_test_loss += loss.item() * labels.size(0)\n",
    "        total_test += labels.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_test += torch.sum(preds == labels).item()\n",
    "\n",
    "test_loss = running_test_loss / total_test\n",
    "test_acc = (correct_test / total_test) * 100\n",
    "print(f\"Test Loss = {test_loss:.4f}, Test Accuracy = {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a676d7-1327-4312-8ec2-1524b7314b11",
   "metadata": {},
   "source": [
    "## Perform data uptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc078f8a-4acb-4919-a57e-36e864feec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 3.8137, Accuracy = 3.17%\n",
      "Epoch 1: Val Loss = 3.8226, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 3.8019, Accuracy = 2.22%\n",
      "Epoch 2: Val Loss = 3.8446, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 3.7995, Accuracy = 3.49%\n",
      "Epoch 3: Val Loss = 3.8990, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 3.7977, Accuracy = 3.81%\n",
      "Epoch 4: Val Loss = 3.8694, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 3.7922, Accuracy = 3.81%\n",
      "Epoch 5: Val Loss = 3.9025, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 3.7883, Accuracy = 3.49%\n",
      "Epoch 6: Val Loss = 3.9166, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 3.7791, Accuracy = 6.03%\n",
      "Epoch 7: Val Loss = 3.9394, Accuracy = 1.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 3.7581, Accuracy = 6.03%\n",
      "Epoch 8: Val Loss = 3.9137, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30: 100%|██████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 3.7442, Accuracy = 6.67%\n",
      "Epoch 9: Val Loss = 3.9722, Accuracy = 1.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 3.6553, Accuracy = 6.67%\n",
      "Epoch 10: Val Loss = 3.9222, Accuracy = 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss = 3.5549, Accuracy = 9.21%\n",
      "Epoch 11: Val Loss = 3.8981, Accuracy = 1.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss = 3.4706, Accuracy = 10.48%\n",
      "Epoch 12: Val Loss = 3.8294, Accuracy = 2.99%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss = 3.3627, Accuracy = 12.06%\n",
      "Epoch 13: Val Loss = 3.7736, Accuracy = 2.99%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss = 3.3002, Accuracy = 12.70%\n",
      "Epoch 14: Val Loss = 3.7233, Accuracy = 1.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss = 3.2551, Accuracy = 14.92%\n",
      "Epoch 15: Val Loss = 3.5931, Accuracy = 5.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss = 3.1830, Accuracy = 13.33%\n",
      "Epoch 16: Val Loss = 3.4900, Accuracy = 5.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Train Loss = 3.0599, Accuracy = 17.14%\n",
      "Epoch 17: Val Loss = 3.3517, Accuracy = 10.45%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Train Loss = 2.9664, Accuracy = 20.95%\n",
      "Epoch 18: Val Loss = 3.3356, Accuracy = 5.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Train Loss = 2.9079, Accuracy = 20.63%\n",
      "Epoch 19: Val Loss = 3.2148, Accuracy = 4.48%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Train Loss = 2.8551, Accuracy = 23.81%\n",
      "Epoch 20: Val Loss = 3.1308, Accuracy = 11.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Train Loss = 2.7734, Accuracy = 20.63%\n",
      "Epoch 21: Val Loss = 3.1259, Accuracy = 7.46%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Train Loss = 2.6681, Accuracy = 24.13%\n",
      "Epoch 22: Val Loss = 3.0332, Accuracy = 23.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Train Loss = 2.6893, Accuracy = 21.27%\n",
      "Epoch 23: Val Loss = 2.9380, Accuracy = 23.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Train Loss = 2.6443, Accuracy = 25.08%\n",
      "Epoch 24: Val Loss = 2.9351, Accuracy = 14.93%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Train Loss = 2.6189, Accuracy = 24.76%\n",
      "Epoch 25: Val Loss = 2.8892, Accuracy = 19.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Train Loss = 2.5216, Accuracy = 27.62%\n",
      "Epoch 26: Val Loss = 2.8218, Accuracy = 29.85%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Train Loss = 2.5351, Accuracy = 26.35%\n",
      "Epoch 27: Val Loss = 2.7380, Accuracy = 28.36%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Train Loss = 2.4618, Accuracy = 28.57%\n",
      "Epoch 28: Val Loss = 2.7861, Accuracy = 17.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Train Loss = 2.4010, Accuracy = 29.52%\n",
      "Epoch 29: Val Loss = 2.6577, Accuracy = 22.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30: 100%|█████████████████████████████████████████████████████████████████████| 40/40 [00:04<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Train Loss = 2.3761, Accuracy = 28.89%\n",
      "Epoch 30: Val Loss = 2.6505, Accuracy = 28.36%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Define model\n",
    "num_classes = 45\n",
    "model = BiometricNet(num_classes=num_classes).to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Slightly lower learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # Reduce LR every 10 epochs\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 30  # Increase to allow better learning\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        left_face = batch['left_face'].to(device)\n",
    "        right_face = batch['right_face'].to(device)\n",
    "        fingerprint = batch['fingerprint'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(left_face, right_face, fingerprint)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_train += (preds == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total_train\n",
    "    epoch_acc = 100 * correct_train / total_train\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            left_face = batch['left_face'].to(device)\n",
    "            right_face = batch['right_face'].to(device)\n",
    "            fingerprint = batch['fingerprint'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(left_face, right_face, fingerprint)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_val += (preds == labels).sum().item()\n",
    "            total_val += labels.size(0)\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "    \n",
    "    val_loss /= total_val\n",
    "    val_acc = 100 * correct_val / total_val\n",
    "    print(f\"Epoch {epoch+1}: Val Loss = {val_loss:.4f}, Accuracy = {val_acc:.2f}%\\n\")\n",
    "\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d7d04c1-72e9-4ffb-b066-7efac95818cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Loss = 3.4716, Test Accuracy = 10.29%\n"
     ]
    }
   ],
   "source": [
    "# Final Test Evaluation\n",
    "model.eval()\n",
    "test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        left_face = batch['left_face'].to(device)\n",
    "        right_face = batch['right_face'].to(device)\n",
    "        fingerprint = batch['fingerprint'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(left_face, right_face, fingerprint)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_test += (preds == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "        test_loss += loss.item() * labels.size(0)\n",
    "\n",
    "test_loss /= total_test\n",
    "test_acc = 100 * correct_test / total_test\n",
    "print(f\"Final Test Loss = {test_loss:.4f}, Test Accuracy = {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb40f63-9c83-4a89-962b-46439618b401",
   "metadata": {},
   "source": [
    "# Applying Transfer Learning for Bimodal Biometrics\n",
    "Since the model is not learning well using raw CNN architecture, we will use pre-trained \n",
    "networks to extract face and fingerprint features, and then train a new classifier on top.\n",
    "\n",
    "Steps for Transfer Learning\n",
    "Use a Pretrained Model for Face Recognition and Fingerprint Recognition\n",
    "\n",
    "    ResNet-18 for Face Recognition\n",
    "\n",
    "    MobileNetV2 for Fingerprint Recognition\n",
    "\n",
    "    Extract fingerprint embeddings.\n",
    "\n",
    "    Combine Features and Train a Final Classifier\n",
    "\n",
    "    Feed extracted embeddings into a fully connected neural network (MLP).\n",
    "\n",
    "    Train only the last layers, while freezing the backbone models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb1566-017f-4450-bc05-7690335a3b9b",
   "metadata": {},
   "source": [
    "## Load Image Paths and Labels\n",
    "Since each subject has multiple images, we will pair each face image with all fingerprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d15b700b-a13b-4800-9a37-ac1b01039253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 4500\n",
      "Unique Classes (Subjects): 45\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the dataset folder (update this if needed)\n",
    "dataset_root = \"Project Assignments\\\\Tumininu Akibowale\\\\IRIS and FINGERPRINT DATASET\"\n",
    "\n",
    "def get_image_paths_and_labels():\n",
    "    image_pairs = []\n",
    "    labels = []\n",
    "\n",
    "    # Loop through each person's folder (1, 2, ..., 45)\n",
    "    for person_id in sorted(os.listdir(dataset_root)):\n",
    "        person_folder = os.path.join(dataset_root, person_id)\n",
    "\n",
    "        if not os.path.isdir(person_folder):\n",
    "            continue  # Skip if not a folder\n",
    "\n",
    "        fingerprint_folder = os.path.join(person_folder, \"Fingerprint\")\n",
    "        left_face_folder = os.path.join(person_folder, \"left\")\n",
    "        right_face_folder = os.path.join(person_folder, \"right\")\n",
    "\n",
    "        # Get all fingerprints\n",
    "        fingerprint_images = [\n",
    "            os.path.join(fingerprint_folder, f)\n",
    "            for f in os.listdir(fingerprint_folder) if f.endswith(\".BMP\")\n",
    "        ]\n",
    "\n",
    "        # Get all left face images\n",
    "        left_face_images = [\n",
    "            os.path.join(left_face_folder, f)\n",
    "            for f in os.listdir(left_face_folder) if f.endswith(\".bmp\")\n",
    "        ]\n",
    "\n",
    "        # Get all right face images\n",
    "        right_face_images = [\n",
    "            os.path.join(right_face_folder, f)\n",
    "            for f in os.listdir(right_face_folder) if f.endswith(\".bmp\")\n",
    "        ]\n",
    "\n",
    "        # Pair each left face image with all fingerprints\n",
    "        for face in left_face_images:\n",
    "            for fingerprint in fingerprint_images:\n",
    "                image_pairs.append((face, fingerprint))\n",
    "                labels.append(int(person_id))\n",
    "\n",
    "        # Pair each right face image with all fingerprints\n",
    "        for face in right_face_images:\n",
    "            for fingerprint in fingerprint_images:\n",
    "                image_pairs.append((face, fingerprint))\n",
    "                labels.append(int(person_id))\n",
    "\n",
    "    return image_pairs, labels\n",
    "\n",
    "# Load data\n",
    "all_image_pairs, all_labels = get_image_paths_and_labels()\n",
    "\n",
    "# Check dataset size\n",
    "print(f\"Total Samples: {len(all_image_pairs)}\")\n",
    "print(f\"Unique Classes (Subjects): {len(set(all_labels))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf50ef2a-fb8c-439f-88ac-44e83f5494bb",
   "metadata": {},
   "source": [
    "## Split Data for Training, Validation, and Testing\n",
    "Since we have multiple samples per subject, we will split at the subject level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "298d1bb7-7308-4385-8cba-7b0b86f518f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Samples: 3100, Val Samples: 700, Test Samples: 700\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Get unique subjects\n",
    "unique_labels = list(set(all_labels))\n",
    "\n",
    "# Split subjects into train (70%), validation (15%), and test (15%)\n",
    "train_subjects, test_subjects = train_test_split(unique_labels, test_size=0.3, random_state=42)\n",
    "val_subjects, test_subjects = train_test_split(test_subjects, test_size=0.5, random_state=42)\n",
    "\n",
    "# Function to filter pairs based on subjects\n",
    "def filter_by_subject(image_pairs, labels, subjects):\n",
    "    return [(img1, img2, label) for (img1, img2), label in zip(image_pairs, labels) if label in subjects]\n",
    "\n",
    "train_data = filter_by_subject(all_image_pairs, all_labels, train_subjects)\n",
    "val_data = filter_by_subject(all_image_pairs, all_labels, val_subjects)\n",
    "test_data = filter_by_subject(all_image_pairs, all_labels, test_subjects)\n",
    "\n",
    "print(f\"Train Samples: {len(train_data)}, Val Samples: {len(val_data)}, Test Samples: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a57d1ffb-7ab1-40f9-b525-1307d31e86da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels in Training Set: {1, 2, 3, 6, 8, 10, 11, 12, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 28, 29, 30, 31, 32, 34, 35, 37, 38, 39, 41, 43, 45}\n",
      "Expected Class Range: 0 to 44\n"
     ]
    }
   ],
   "source": [
    "# Extract all labels from dataset\n",
    "num_classes = 45\n",
    "train_labels = [label for _, _, label in train_data]\n",
    "val_labels = [label for _, _, label in val_data]\n",
    "test_labels = [label for _, _, label in test_data]\n",
    "\n",
    "print(\"Unique Labels in Training Set:\", set(train_labels))\n",
    "print(\"Expected Class Range: 0 to\", num_classes - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "803bd6b0-2da1-4131-9158-45056d840ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels in Training Set: {0, 1, 2, 5, 7, 9, 10, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 27, 28, 29, 30, 31, 33, 34, 36, 37, 38, 40, 42, 44}\n"
     ]
    }
   ],
   "source": [
    "train_labels = [label - 1 for label in train_labels]\n",
    "val_labels = [label - 1 for label in val_labels]\n",
    "test_labels = [label - 1 for label in test_labels]\n",
    "\n",
    "print(\"Unique Labels in Training Set:\", set(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff938474-4444-422b-8dd6-21383ee28e0d",
   "metadata": {},
   "source": [
    "## Create PyTorch Dataset Class\n",
    "We will process both face and fingerprint images and use ResNet-18 (for faces) and MobileNetV2 (for fingerprints) as feature extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4aae2c2f-01e4-4ea4-95cb-65d62e2e2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load pre-trained models\n",
    "face_model = models.resnet18(pretrained=True)\n",
    "# Modify MobileNetV2 to include global average pooling\n",
    "fingerprint_model = models.mobilenet_v2(pretrained=True)\n",
    "fingerprint_model.classifier = nn.Identity()  # Remove final classification layer\n",
    "\n",
    "# Remove final classification layers (we only need feature extractors)\n",
    "face_model = nn.Sequential(*list(face_model.children())[:-1])\n",
    "fingerprint_model = nn.Sequential(*list(fingerprint_model.children())[:-1])\n",
    "\n",
    "# Set models to evaluation mode (no training)\n",
    "face_model.eval()\n",
    "fingerprint_model.eval()\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "def extract_embedding(image_path, model):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image)\n",
    "\n",
    "    # Apply Global Average Pooling if necessary\n",
    "    if len(embedding.shape) > 2:  # If it's not a 1D tensor\n",
    "        embedding = torch.nn.functional.adaptive_avg_pool2d(embedding, (1, 1))\n",
    "\n",
    "    return embedding.view(-1)  # Flatten to 1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf5c028-d033-4509-8fbb-134896080e08",
   "metadata": {},
   "source": [
    "## Define PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0b0b892-2434-4b0e-8be2-8068589432ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Embedding Size: torch.Size([512])\n",
      "Fingerprint Embedding Size: torch.Size([1280])\n",
      "Final Feature Vector Size: torch.Size([1792])\n"
     ]
    }
   ],
   "source": [
    "sample_face, sample_fingerprint, _ = train_data[0]\n",
    "\n",
    "face_embedding = extract_embedding(sample_face, face_model)\n",
    "fingerprint_embedding = extract_embedding(sample_fingerprint, fingerprint_model)\n",
    "\n",
    "print(\"Face Embedding Size:\", face_embedding.shape)  # Should be [512]\n",
    "print(\"Fingerprint Embedding Size:\", fingerprint_embedding.shape)  # Should be [1280]\n",
    "\n",
    "# Concatenate embeddings\n",
    "features = torch.cat((face_embedding, fingerprint_embedding), dim=0)\n",
    "print(\"Final Feature Vector Size:\", features.shape)  # Should be [1792]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9b863-17a3-4414-a7f6-eafacd000094",
   "metadata": {},
   "source": [
    "## Modify Dataset Class\n",
    "Instead of feeding raw images into CNN, we now extract embeddings from the pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e0365d37-f63e-46c2-a6ec-3e38a9d1e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BimodalBiometricDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths  # List of (face_path, fingerprint_path)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        face_path, fingerprint_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Extract embeddings\n",
    "        face_embedding = extract_embedding(face_path, face_model)\n",
    "        fingerprint_embedding = extract_embedding(fingerprint_path, fingerprint_model)\n",
    "\n",
    "        # Concatenate both embeddings\n",
    "        features = torch.cat((face_embedding, fingerprint_embedding), dim=0)\n",
    "\n",
    "        return {'features': features, 'label': torch.tensor(label, dtype=torch.long)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79a1b2-56ff-4eda-ab62-025fc0afc217",
   "metadata": {},
   "source": [
    "## Define a New Classifier\n",
    "Since the feature extractors are frozen, we now train an MLP classifier on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "228e602f-4336-4c0d-b02b-bc92f9ea37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiometricClassifier(nn.Module):\n",
    "    def __init__(self, feature_size, num_classes):\n",
    "        super(BiometricClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0063fe-d09d-420a-893b-73cdea1fb8df",
   "metadata": {},
   "source": [
    "## Train the Classifier\n",
    "Since embeddings are already extracted, we train only the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c04170f5-e4d9-4ba5-aa76-6cf2e9fe11da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m classifier\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     38\u001b[0m running_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     41\u001b[0m     features \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[69], line 10\u001b[0m, in \u001b[0;36mBimodalBiometricDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 10\u001b[0m     face_path, fingerprint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[0;32m     11\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Extract embeddings\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloaders\n",
    "train_dataset = BimodalBiometricDataset(train_data, train_labels)\n",
    "val_dataset = BimodalBiometricDataset(val_data, val_labels)\n",
    "test_dataset = BimodalBiometricDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Define MLP Classifier\n",
    "class BiometricClassifier(nn.Module):\n",
    "    def __init__(self, feature_size, num_classes):\n",
    "        super(BiometricClassifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(feature_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Set up model\n",
    "feature_size = 512 + 1280  # ResNet-18 output (512) + MobileNetV2 output (1280)\n",
    "num_classes = 45\n",
    "classifier = BiometricClassifier(feature_size, num_classes).to(\"cuda\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        features = batch['features'].to(\"cuda\")\n",
    "        labels = batch['label'].to(\"cuda\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy tracking\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item() * labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Accuracy = {train_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ff8b5-34e0-43ba-b62f-d968980f04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation\n",
    "classifier.eval()\n",
    "test_loss, correct_test, total_test = 0.0, 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        features = batch['features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = classifier(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_test += (preds == labels).sum().item()\n",
    "        total_test += labels.size(0)\n",
    "        test_loss += loss.item() * labels.size(0)\n",
    "\n",
    "test_loss /= total_test\n",
    "test_acc = 100 * correct_test / total_test\n",
    "print(f\"Final Test Loss = {test_loss:.4f}, Test Accuracy = {test_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
