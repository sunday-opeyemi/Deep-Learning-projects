{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "73526b82-5ecd-4ae0-80b6-31190599a3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>stance</th>\n",
       "      <th>argumentation scheme</th>\n",
       "      <th>argument</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Euthanasia</td>\n",
       "      <td>in favor</td>\n",
       "      <td>position to know</td>\n",
       "      <td>{\\n  \"major premise\": \"Medical professionals a...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mandatory vaccination in pandemic</td>\n",
       "      <td>against</td>\n",
       "      <td>expert opinion</td>\n",
       "      <td>{\\n  \"major premise\": \"Dr. John Smith is an ex...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Physical appearance for personal success</td>\n",
       "      <td>against</td>\n",
       "      <td>expert opinion</td>\n",
       "      <td>{\\n  \"major premise\": \"Dr. John Smith is an ex...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intermittent fasting</td>\n",
       "      <td>in favor</td>\n",
       "      <td>expert opinion</td>\n",
       "      <td>{\\n  \"major premise\": \"Dr. Jack Kevorkian is a...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Capital punishment</td>\n",
       "      <td>against</td>\n",
       "      <td>expert opinion</td>\n",
       "      <td>{\\n  \"major premise\": \"Dr. John Smith is an ex...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      topic    stance argumentation scheme  \\\n",
       "0                                Euthanasia  in favor     position to know   \n",
       "1         Mandatory vaccination in pandemic   against       expert opinion   \n",
       "2  Physical appearance for personal success   against       expert opinion   \n",
       "3                      Intermittent fasting  in favor       expert opinion   \n",
       "4                        Capital punishment   against       expert opinion   \n",
       "\n",
       "                                            argument label  \n",
       "0  {\\n  \"major premise\": \"Medical professionals a...   yes  \n",
       "1  {\\n  \"major premise\": \"Dr. John Smith is an ex...   yes  \n",
       "2  {\\n  \"major premise\": \"Dr. John Smith is an ex...   yes  \n",
       "3  {\\n  \"major premise\": \"Dr. Jack Kevorkian is a...   yes  \n",
       "4  {\\n  \"major premise\": \"Dr. John Smith is an ex...   yes  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "with open('nlas-multi2.json', 'r', encoding='latin-1') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert to a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data['eng']).T\n",
    "\n",
    "# Display the DataFrame to inspect it\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4a20b105-51ae-442c-9e88-2cb81eb20d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "\n",
    "# Load GPT-2 tokenizer and set padding token to the eos_token\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load GPT-2 model for sequence classification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Make sure the model knows the padding token ID\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2249e7a3-666d-48e0-a7bf-47949d8f88ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments\n",
    "import json\n",
    "\n",
    "# Custom Dataset Class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, json_file, tokenizer, max_length=512):\n",
    "        with open(json_file, 'r', encoding='latin-1') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "\n",
    "        for key in self.data['eng']:\n",
    "            item = self.data['eng'][key]\n",
    "            topic = item['topic']\n",
    "            stance = item['stance']\n",
    "            scheme = item['argumentation scheme']\n",
    "            argument = item['argument']\n",
    "            label = item['label']\n",
    "\n",
    "            text = f\"Topic: {topic}\\nStance: {stance}\\nScheme: {scheme}\\nArgument: {argument}\\n\"\n",
    "            self.samples.append((text, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.samples[idx]\n",
    "\n",
    "        # Tokenize the input text with padding\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',  # Ensure padding to max_length\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        label = torch.tensor(1 if label == \"yes\" else 0, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the dataset\n",
    "dataset = CustomDataset('nlas-multi2.json', tokenizer)\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n",
    "\n",
    "# Load GPT-2 model for sequence classification\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ec349eb-cbff-4b4c-8fc0-2872f8c67cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model\\\\tokenizer_config.json',\n",
       " './trained_model\\\\special_tokens_map.json',\n",
       " './trained_model\\\\vocab.json',\n",
       " './trained_model\\\\merges.txt',\n",
       " './trained_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,  # Set a high number of epochs; early stopping will determine when to stop\n",
    "    per_device_train_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",  # Evaluate at intervals\n",
    "    eval_steps=100,               # Evaluate every 100 steps\n",
    "    save_total_limit=1,           # Save only the best model\n",
    "    load_best_model_at_end=True   # Load the best model when training ends\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with early stopping\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop after 3 non-improving evals\n",
    ")\n",
    "\n",
    "# Optionally, save the trained model\n",
    "model.save_pretrained('./trained_model')\n",
    "tokenizer.save_pretrained('./trained_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53d337b0-48e3-4cb3-96f0-e1055963bb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The argument you've presented seems valid. Given the context, it's evident that 'Artificial intelligence is transforming industries across the globe.' is a point of discussion that holds merit. However, it's important to consider all facets of the topic to fully understand its implications.\n",
      "-------------------------------------------------------------------------\n",
      "The reason behind this is multifaceted. Generally, when discussing topics like 'why is climate change a pressing issue', several factors come into play. It could be influenced by various circumstances, including historical context, current trends, or technological developments.\n",
      "-------------------------------------------------------------------------\n",
      "The process or impact of 'how do vaccines work' can vary depending on several factors. It involves understanding the mechanisms behind it, which may include policy implications, societal influences, and technological advancements.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_response(model, tokenizer, text, max_length=512):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Move tensors to the correct device (CPU or GPU)\n",
    "    input_ids = inputs['input_ids'].to(model.device)\n",
    "    attention_mask = inputs['attention_mask'].to(model.device)\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Get the predicted label (0 or 1)\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # Determine the type of input (statement or question)\n",
    "    if text.strip().endswith('?'):\n",
    "        # It's a question\n",
    "        if \"why\" in text.lower():\n",
    "            response = (\n",
    "                f\"The reason behind this is multifaceted. Generally, when discussing topics like '{text.strip('?').lower()}', \"\n",
    "                f\"several factors come into play. It could be influenced by various circumstances, including historical context, current trends, or technological developments.\"\n",
    "            )\n",
    "        elif \"how\" in text.lower():\n",
    "            response = (\n",
    "                f\"The process or impact of '{text.strip('?').lower()}' can vary depending on several factors. \"\n",
    "                f\"It involves understanding the mechanisms behind it, which may include policy implications, societal influences, and technological advancements.\"\n",
    "            )\n",
    "        else:\n",
    "            response = (\n",
    "                f\"That's an interesting question. When we consider '{text.strip('?').lower()}', \"\n",
    "                f\"it opens up a discussion on multiple dimensions, including social, economic, and environmental aspects.\"\n",
    "            )\n",
    "    else:\n",
    "        # It's a statement (argument)\n",
    "        if predicted_label == 1:\n",
    "            response = (\n",
    "                f\"The argument you've presented seems valid. Given the context, it's evident that '{text}' is a point of discussion that holds merit. \"\n",
    "                f\"However, it's important to consider all facets of the topic to fully understand its implications.\"\n",
    "            )\n",
    "        else:\n",
    "            response = (\n",
    "                f\"While your argument touches on an important issue, it might need more context. Although '{text}', \"\n",
    "                f\"there are several factors that suggest the impact or relevance might not be as significant as it appears. \"\n",
    "                f\"Further analysis or evidence may be required to fully substantiate the argument.\"\n",
    "            )\n",
    "\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "# Load the trained model and tokenizer\n",
    "model = GPT2ForSequenceClassification.from_pretrained('./trained_model')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./trained_model')\n",
    "\n",
    "# Example input texts\n",
    "input_text_1 = \"Artificial intelligence is transforming industries across the globe.\"\n",
    "input_text_2 = \"Why is climate change a pressing issue?\"\n",
    "input_text_3 = \"How do vaccines work?\"\n",
    "\n",
    "# Generate responses\n",
    "response_1 = generate_response(model, tokenizer, input_text_1)\n",
    "response_2 = generate_response(model, tokenizer, input_text_2)\n",
    "response_3 = generate_response(model, tokenizer, input_text_3)\n",
    "\n",
    "print(response_1)\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "print(response_2)\n",
    "print(\"-------------------------------------------------------------------------\")\n",
    "print(response_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1303ca0-e7ff-4fe4-8b2a-b29ff955427e",
   "metadata": {},
   "source": [
    "# Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd7240a3-ea7f-4743-819c-813cc860be91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic': [], 'instance': []}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pretrained NER model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Create an NER pipeline\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def extract_entities(text):\n",
    "    ner_results = nlp(text)\n",
    "    entities = {\"topic\": [], \"instance\": []}\n",
    "\n",
    "    # Iterate through the NER results\n",
    "    for entity in ner_results:\n",
    "        # Check for the key 'entity' instead of 'entity_group'\n",
    "        entity_type = entity['entity']\n",
    "        \n",
    "        # Example entity labels for topic and instance (these might vary based on your model)\n",
    "        if \"TOPIC\" in entity_type.upper():\n",
    "            entities['topic'].append(entity['word'])\n",
    "        elif \"INSTANCE\" in entity_type.upper():\n",
    "            entities['instance'].append(entity['word'])\n",
    "            \n",
    "    return entities\n",
    "\n",
    "# Test the function with an example text\n",
    "text = \"What are the arguments related to climate change?\"\n",
    "entities = extract_entities(text)\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c77f034e-66be-4af5-a815-870c27febe57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "# Load the JSON data\n",
    "with open('nlas-multi2.json', 'r', encoding='latin-1') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "class ArgumentationDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.inputs.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "def preprocess_data(data):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for key, item in data.items():\n",
    "        argument = item['argument']\n",
    "        label = item['label']\n",
    "        \n",
    "        # Ensure that the argument is a string\n",
    "        if isinstance(argument, str):\n",
    "            texts.append(argument)\n",
    "            labels.append(1 if label == 'yes' else 0)\n",
    "\n",
    "    # Tokenize all texts at once\n",
    "    inputs = tokenizer(texts, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    return inputs, labels\n",
    "\n",
    "# Assuming `data` is already loaded from the JSON file\n",
    "inputs, labels = preprocess_data(data['eng'])\n",
    "\n",
    "# Create the datasets\n",
    "dataset = ArgumentationDataset(inputs, labels)\n",
    "# eval_dataset = ArgumentationDataset(inputs, labels)  # In a real case, you should split the data\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "eval_size = len(dataset) - train_size\n",
    "train_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c82fe526-a70d-4a7c-95e9-b0364750ba70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/400 00:56 < 3:05:15, 0.04 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 27\u001b[0m\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Optional callbacks\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Start fine-tuning\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1933\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1934\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1935\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1936\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1937\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2268\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2268\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2271\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2273\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2274\u001b[0m ):\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   3309\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3311\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:3338\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   3336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3337\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3338\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   3339\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3340\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1781\u001b[0m, in \u001b[0;36mGPT2ForSequenceClassification.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1779\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1781\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m   1782\u001b[0m     input_ids,\n\u001b[0;32m   1783\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1784\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1785\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1786\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1787\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1788\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1789\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1790\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1791\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1792\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1793\u001b[0m )\n\u001b[0;32m   1794\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1795\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1235\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1223\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1224\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1225\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1232\u001b[0m         output_attentions,\n\u001b[0;32m   1233\u001b[0m     )\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1235\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m block(\n\u001b[0;32m   1236\u001b[0m         hidden_states,\n\u001b[0;32m   1237\u001b[0m         layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m   1238\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1239\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask[i],\n\u001b[0;32m   1240\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1241\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m   1242\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1243\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1244\u001b[0m     )\n\u001b[0;32m   1246\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:720\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    718\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    719\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 720\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[0;32m    721\u001b[0m     hidden_states,\n\u001b[0;32m    722\u001b[0m     layer_past\u001b[38;5;241m=\u001b[39mlayer_past,\n\u001b[0;32m    723\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    724\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    725\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    726\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    727\u001b[0m )\n\u001b[0;32m    728\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    729\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:664\u001b[0m, in \u001b[0;36mGPT2SdpaAttention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    661\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;66;03m# Final projection\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n\u001b[0;32m    665\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(attn_output)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, present, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:104\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    103\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[1;32m--> 104\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39maddmm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m    105\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Initialize TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    # load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Optional callbacks\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fe0859db-034b-4540-86b5-c4d5cb378990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model\\\\tokenizer_config.json',\n",
       " './trained_model\\\\special_tokens_map.json',\n",
       " './trained_model\\\\vocab.txt',\n",
       " './trained_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optionally, save the trained model\n",
    "model.save_pretrained('./trained_model')\n",
    "tokenizer.save_pretrained('./trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92ec892-9c07-4853-8b5d-446b2e1427d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 56\u001b[0m\n\u001b[0;32m     48\u001b[0m user_questions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould we implement stricter gun control laws?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIs climate change a real threat?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould taxes be increased on the wealthy?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDo video games cause violence?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m ]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m user_questions:\n\u001b[1;32m---> 56\u001b[0m     response \u001b[38;5;241m=\u001b[39m chatbot_response(question, model, data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "import random \n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def preprocess_user_input(user_input):\n",
    "    # Use a larger max_length if possible, or dynamic padding\n",
    "    inputs = tokenizer(user_input, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def predict_argument(model, inputs):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "    return predicted_class_id\n",
    "\n",
    "def get_argument_from_label(predicted_class_id, dataset):\n",
    "    arguments = []\n",
    "    for key, item in dataset.items():\n",
    "        label = item['label']\n",
    "        if (label == 'yes' and predicted_class_id == 1) or (label == 'no' and predicted_class_id == 0):\n",
    "            arguments.append(item['argument'])\n",
    "    \n",
    "    if arguments:\n",
    "        # Randomly select an argument to add variety\n",
    "        return random.choice(arguments)\n",
    "    else:\n",
    "        return \"I couldn't find a specific argument for that question.\"\n",
    "\n",
    "\n",
    "def chatbot_response(user_input, model, dataset):\n",
    "    # Preprocess the user's input\n",
    "    inputs = preprocess_user_input(user_input)\n",
    "    \n",
    "    # Get the predicted argument ID\n",
    "    predicted_class_id = predict_argument(model, inputs)\n",
    "    \n",
    "    # Get the corresponding argument from the dataset\n",
    "    response = get_argument_from_label(predicted_class_id, dataset)\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example questions to test different topics\n",
    "user_questions = [\n",
    "    \"Should we implement stricter gun control laws?\",\n",
    "    \"Is climate change a real threat?\",\n",
    "    \"Should taxes increased be on the wealthy?\",\n",
    "    \"Do video games cause violence?\"\n",
    "]\n",
    "\n",
    "for question in user_questions:\n",
    "    response = chatbot_response(question, model, data['eng'])\n",
    "    print(f\"User: {question}\")\n",
    "    print(f\"Chatbot: {response}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66fe2e-0999-437f-a31e-f38816780a26",
   "metadata": {},
   "source": [
    "# Alternative 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "96805dd1-ee8f-48a0-a2bb-c3ce31ccb0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Output: []\n",
      "Checking topic: euthanasia against climate change\n",
      "Checking topic: mandatory vaccination in pandemic against climate change\n",
      "Checking topic: physical appearance for personal success against climate change\n",
      "Checking topic: intermittent fasting against climate change\n",
      "Checking topic: capital punishment against climate change\n",
      "Checking topic: animal testing against climate change\n",
      "Checking topic: climate change against climate change\n",
      "Here's what I found: {\n",
      "  \"major premise\": \"Dr. John Smith is an expert in medical ethics containing proposition that climate change undermines the value of human life.\",\n",
      "  \"minor premise\": \"Dr. Smith asserts that climate change undermines the value of human life.\",\n",
      "  \"conclusion\": \"Climate change undermines the value of human life.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "# Load the NER pipeline from Hugging Face\n",
    "# ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "# ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\")\n",
    "# ner_pipeline = pipeline(\"ner\", model=\"Davlan/xlm-roberta-large-ner-english\")\n",
    "ner_pipeline = pipeline(\"ner\", model=\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Load NLAS data\n",
    "with open('nlas-multi2.json', 'r') as file:\n",
    "    nlas_data = json.load(file)\n",
    "\n",
    "# Extract all topics from NLAS for keyword matching    \n",
    "nlas_topics = [item['topic'].lower().strip() for item in nlas_data[\"eng\"].values()]\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract entities from user input using NER and keyword matching.\"\"\"\n",
    "    entities = ner_pipeline(text)\n",
    "    print(\"NER Output:\", entities)  # Debugging line\n",
    "    \n",
    "    # Extract topics from NER output\n",
    "    topics = [entity['word'].lower().strip() for entity in entities if entity['entity'] == 'I-MISC' or entity['entity'] == 'I-ORG']\n",
    "\n",
    "    # If NER fails, fall back to keyword matching\n",
    "    if not topics:\n",
    "        topics = [topic for topic in nlas_topics if topic in text.lower()]\n",
    "    \n",
    "    return topics\n",
    "\n",
    "def find_argument(topic, stance=None):\n",
    "    \"\"\"Find an argument in the NLAS data based on topic and optional stance.\"\"\"\n",
    "    for item in nlas_data[\"eng\"].values():\n",
    "        print(f\"Checking topic: {item['topic'].lower().strip()} against {topic}\")  # Debugging line\n",
    "        if item['topic'].lower().strip() == topic:\n",
    "            if stance is None or item['stance'].lower().strip() == stance.lower().strip():\n",
    "                return item['argument']\n",
    "    return None\n",
    "\n",
    "def get_argument_for_user_input(user_input):\n",
    "    # Step 1: Extract topic from user input using NER and keyword matching\n",
    "    topics = extract_entities(user_input)\n",
    "    if not topics:\n",
    "        return \"I'm sorry, I couldn't find any relevant topics in your query.\"\n",
    "\n",
    "    # Assuming the first detected topic is the most relevant\n",
    "    topic = topics[0]\n",
    "    \n",
    "    # Optional: Infer stance based on keywords\n",
    "    stance = None\n",
    "    if any(keyword in user_input.lower() for keyword in ['support', 'favor', 'pro', 'agree']):\n",
    "        stance = 'in favor'\n",
    "    elif any(keyword in user_input.lower() for keyword in ['against', 'oppose', 'con', 'disagree']):\n",
    "        stance = 'against'\n",
    "\n",
    "    # Step 2: Retrieve argument from NLAS database\n",
    "    argument = find_argument(topic, stance)\n",
    "\n",
    "    if argument:\n",
    "        return argument\n",
    "    else:\n",
    "        return f\"Sorry, I couldn't find an argument on the topic '{topic}' in the database.\"\n",
    "\n",
    "def chatbot_response(user_input):\n",
    "    argument = get_argument_for_user_input(user_input)\n",
    "    return f\"Here's what I found: {argument}\"\n",
    "\n",
    "# Example user interaction\n",
    "user_input = \"Is climate change a real threat?\"\n",
    "response = chatbot_response(user_input)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2939af21-0351-4b57-b2f8-3a830f027d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER Output: []\n",
      "Checking topic: euthanasia against climate change\n",
      "Checking topic: mandatory vaccination in pandemic against climate change\n",
      "Checking topic: physical appearance for personal success against climate change\n",
      "Checking topic: intermittent fasting against climate change\n",
      "Checking topic: capital punishment against climate change\n",
      "Checking topic: animal testing against climate change\n",
      "Checking topic: climate change against climate change\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import torch\n",
    "\n",
    "# Function to handle user input and display the response\n",
    "def send_message():\n",
    "    user_input = user_input_entry.get(\"1.0\", \"end-1c\")  # Get the text from the input box\n",
    "    user_input_entry.delete(\"1.0\", \"end\")  # Clear the input box\n",
    "    if user_input.strip():\n",
    "        chat_display.config(state=tk.NORMAL)\n",
    "        chat_display.insert(tk.END, \"User: \" + user_input + \"\\n\\n\")\n",
    "        chat_display.config(state=tk.DISABLED)\n",
    "\n",
    "        response = response = response = chatbot_response(user_input)\n",
    "        chat_display.config(state=tk.NORMAL)\n",
    "        chat_display.insert(tk.END, \"Chatbot: \" + response + \"\\n\\n\")\n",
    "        chat_display.config(state=tk.DISABLED)\n",
    "\n",
    "# Create the main application window\n",
    "root = tk.Tk()\n",
    "root.title(\"Argumentative Chatbot\")\n",
    "root.geometry(\"670x480\")\n",
    "\n",
    "# Create a display area for the chat conversation\n",
    "chat_display = scrolledtext.ScrolledText(root, height=20, width=80, state=tk.DISABLED, wrap=tk.WORD)\n",
    "chat_display.grid(column=0, row=0, padx=10, pady=10, columnspan=2)\n",
    "chat_display.configure(state='disabled')\n",
    "\n",
    "entry_label = tk.Label(root, text=\"Ask your question:\")\n",
    "entry_label.grid(column=0, row=1, padx=10, pady=10)\n",
    "\n",
    "user_input_entry = tk.Text(root, height=2, width=70)\n",
    "user_input_entry.grid(column=0, row=2, padx=10, pady=10)\n",
    "\n",
    "send_button = tk.Button(root, text=\"Send\", command=send_message)\n",
    "send_button.grid(column=1, row=3, padx=10, pady=10)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72df2f-4acc-4135-a2f6-b61878ee3115",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
