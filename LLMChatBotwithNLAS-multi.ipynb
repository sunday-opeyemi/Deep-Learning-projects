{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fa56e-c5c8-4402-9d81-8891f4d37192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Question Answering (QA) Chatbot with a natural language argument generation and integrating the \n",
    "# NLAS-multi corpus is a complex task that involves several steps. Hereâ€™s a structured plan to implement the chatbot:\n",
    "\n",
    "# Step 1: Set Up the Environment\n",
    "# 1. Install Necessary Libraries:\n",
    "#    - For natural language processing: `transformers`, `spaCy`, `datasets`, `sentence-transformers`, etc.\n",
    "#    - For creating a web interface: `streamlit`.\n",
    "\n",
    "# bash\n",
    "# pip install transformers spacy datasets sentence-transformers streamlit\n",
    "\n",
    "\n",
    "# 2. Download NLAS-multi Corpus:\n",
    "#    - Access the dataset from Zenodo and download it.\n",
    "\n",
    "# Step 2: Pre-process the NLAS-multi Corpus\n",
    "# 1. Load the Corpus:\n",
    "#    - Load the dataset and examine its structure.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "nlas_multi_path = 'path_to_nlas_multi.csv'\n",
    "nlas_multi = pd.read_csv(nlas_multi_path)\n",
    "print(nlas_multi.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70349bb3-94a2-430d-add3-a64c12266f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Clean and Organize the Data:\n",
    "#    - Ensure the dataset is clean and organized for efficient access during inference.\n",
    "\n",
    "### Step 3: Develop Named Entity Recognition (NER) Model\n",
    "# 1. Train/Fine-tune NER Model:\n",
    "#    - Use spaCy or Hugging Face transformers to fine-tune an NER model for detecting entities relevant to the questions.\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load a pre-trained model and fine-tune it\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add custom entities as needed\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"TOPIC\")\n",
    "ner.add_label(\"EXPERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e62da4-f9a4-419c-a530-062fad2bb3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Entity Extraction:\n",
    "#    - Implement a function to extract entities from user questions.\n",
    "\n",
    "def extract_entities(question):\n",
    "    doc = nlp(question)\n",
    "    entities = {ent.label_: ent.text for ent in doc.ents}\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc0eae4-d3a1-488e-be0b-c843c2cf8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Argument Retrieval\n",
    "# 1. Query the NLAS-multi Corpus:\n",
    "#    - Implement logic to retrieve relevant arguments from the corpus based on the identified entities.\n",
    "\n",
    "def get_argument(entities, stance=None):\n",
    "    # Query the dataset based on entities\n",
    "    topic = entities.get(\"TOPIC\")\n",
    "    expert = entities.get(\"EXPERT\")\n",
    "    \n",
    "    if topic:\n",
    "        filtered_df = nlas_multi[nlas_multi['topic'].str.contains(topic, case=False)]\n",
    "        if stance:\n",
    "            filtered_df = filtered_df[filtered_df['stance'].str.contains(stance, case=False)]\n",
    "        # Further refine based on expert if available\n",
    "        if expert:\n",
    "            filtered_df = filtered_df[filtered_df['expert'].str.contains(expert, case=False)]\n",
    "        \n",
    "        return filtered_df.iloc[0]['argument'] if not filtered_df.empty else \"No argument found.\"\n",
    "    else:\n",
    "        return \"Please provide more details.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e14dd3-25fb-41ad-a8d1-610e4a8161f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example use\n",
    "entities = extract_entities(\"Is there an expert positioned in favour of climate change?\")\n",
    "argument = get_argument(entities, stance=\"favour\")\n",
    "print(argument)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb3bc1-e849-4779-8dc0-36ed12cea79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5: Create a User Interface\n",
    "# 1. Build a Streamlit App:\n",
    "#    - Create an interactive web interface using Streamlit.\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"QA Chatbot with Argument Generation\")\n",
    "\n",
    "user_input = st.text_input(\"Ask a question:\")\n",
    "if user_input:\n",
    "    entities = extract_entities(user_input)\n",
    "    stance = \"favour\" if \"favour\" in user_input else \"against\" if \"against\" in user_input else None\n",
    "    argument = get_argument(entities, stance)\n",
    "    st.write(\"Argument:\", argument)\n",
    "\n",
    "\n",
    "# 2. **Run the Streamlit App**:\n",
    "streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32af8d-7b6c-4c5b-85aa-9e63c46f94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from langdetect import detect\n",
    "nlp={}    \n",
    "for lang in [\"en\", \"es\", \"pt\", \"ru\"]: # Fill in the languages you want, hopefully they are supported by spacy.\n",
    "    if lang == \"en\":\n",
    "        nlp[lang]=spacy.load(lang + '_core_web_lg')\n",
    "    else: \n",
    "        nlp[lang]=spacy.load(lang + '_core_news_lg')\n",
    "\n",
    "def entites(text):\n",
    "     lang = detect(text)\n",
    "     try:\n",
    "         nlp2 =nlp[lang]\n",
    "     except KeyError:\n",
    "         return Exception(lang + \" model is not loaded\")\n",
    "     return [(str(x), x.label_) for x in nlp2(str(text)).ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa341a0a-9909-4e43-8872-31c36ba2ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sure, let's break down the implementation of the Question Answering (QA) Chatbot that utilizes the NLAS-multi corpus for generating arguments.\n",
    "The project will be divided into several stages:\n",
    "\n",
    "### 1. Implementing the QA Chatbot\n",
    "\n",
    "We will develop a chatbot capable of answering questions by generating arguments based on the NLAS-multi corpus. The chatbot will perform\n",
    "the following tasks:\n",
    "\n",
    "- **Receive and process natural language inputs.**\n",
    "- **Identify key entities and concepts using a Named Entity Recognition (NER) model.**\n",
    "- **Generate arguments based on the identified entities and the NLAS-multi corpus.**\n",
    "- **Respond to the user in natural language.**\n",
    "\n",
    "### 2. Integrating the NLAS-multi Corpus\n",
    "\n",
    "The NLAS-multi corpus is a collection of automatically generated arguments. We will leverage this corpus to provide well-formed arguments \n",
    "    in response to user questions.\n",
    "\n",
    "#### a. Training a Named Entity Recognition (NER) Model\n",
    "\n",
    "The NER model will identify important entities in the user question, such as topics, experts, and stance \n",
    "indicators (e.g., \"in favour\" or \"against\"). This preprocessing step is crucial for generating relevant arguments.\n",
    "\n",
    "#### b. Input and Output Specifications\n",
    "\n",
    "- **Input:** A user question in natural language (e.g., \"Should [Topic] be allowed?\", \"Is there an [Expert] positioned in favour/against [Topic]?\").\n",
    "- **Output:** A well-formed argument based on the input question and entities.\n",
    "\n",
    "### 3. Developing a User-Friendly Interface\n",
    "\n",
    "We will create an interface that allows users to interact with the chatbot easily. This interface will likely be web-based for accessibility.\n",
    "\n",
    "### Implementation Steps\n",
    "\n",
    "#### Step 1: Setting Up the Environment\n",
    "\n",
    "1. **Download and Prepare the NLAS-multi Corpus:**\n",
    "   - Download the corpus from [Zenodo](https://zenodo.org/records/8364002).\n",
    "   - Preprocess the corpus to structure it for easy retrieval.\n",
    "\n",
    "2. **Train the NER Model:**\n",
    "   - Use a pre-trained model such as spaCy, fine-tuned on a relevant dataset to recognize entities like topics, experts, and stance indicators.\n",
    "\n",
    "3. **Develop the Chatbot Backend:**\n",
    "   - Implement the logic to process user inputs, use the NER model for preprocessing, and retrieve/generate arguments from the NLAS-multi corpus.\n",
    "\n",
    "4. **Create the User Interface:**\n",
    "   - Develop a simple web interface using a framework like Flask (for backend) and HTML/CSS/JavaScript (for frontend).\n",
    "\n",
    "### Detailed Implementation\n",
    "\n",
    "#### 1. Downloading and Preparing the NLAS-multi Corpus\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Download the NLAS-multi corpus from Zenodo (assuming it's already downloaded)\n",
    "\n",
    "# Load the corpus\n",
    "with open('path_to_nlas_multi_corpus.json', 'r') as file:\n",
    "    nlas_corpus = json.load(file)\n",
    "\n",
    "\n",
    "#### 2. Training the NER Model\n",
    "\n",
    "Using spaCy to train the NER model:\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import random\n",
    "\n",
    "# Load a pre-existing spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define training data\n",
    "TRAIN_DATA = [\n",
    "    (\"Should vaccination be mandatory?\", {\"entities\": [(7, 17, \"TOPIC\")]}),\n",
    "    (\"Is there an expert in favour of climate change?\", {\"entities\": [(11, 16, \"EXPERT\"), (31, 44, \"TOPIC\")]}),\n",
    "    # More training data\n",
    "]\n",
    "\n",
    "# Convert training data to spaCy's format\n",
    "def create_training_data(train_data):\n",
    "    db = DocBin()\n",
    "    for text, annot in train_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            ents.append(span)\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return db\n",
    "\n",
    "train_data = create_training_data(TRAIN_DATA)\n",
    "train_data.to_disk(\"./train.spacy\")\n",
    "\n",
    "# Fine-tune the NER model\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Disabling other pipelines to only train NER\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    optimizer = nlp.resume_training()\n",
    "    for i in range(20):  # Number of iterations\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            nlp.update([text], [annotations], drop=0.5, losses=losses)\n",
    "        print(losses)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "nlp.to_disk(\"./fine_tuned_model\")\n",
    "```\n",
    "\n",
    "#### 3. Developing the Chatbot Backend\n",
    "\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "import spacy\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the fine-tuned NER model\n",
    "nlp = spacy.load(\"./fine_tuned_model\")\n",
    "\n",
    "@app.route('/get_argument', methods=['POST'])\n",
    "def get_argument():\n",
    "    user_input = request.json.get('question')\n",
    "    doc = nlp(user_input)\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = {ent.label_: ent.text for ent in doc.ents}\n",
    "    \n",
    "    # Generate argument based on entities and corpus\n",
    "    argument = generate_argument(entities)\n",
    "    \n",
    "    return jsonify({\"argument\": argument})\n",
    "\n",
    "def generate_argument(entities):\n",
    "    # Simplified function to retrieve argument from the corpus\n",
    "    topic = entities.get(\"TOPIC\")\n",
    "    expert = entities.get(\"EXPERT\")\n",
    "    stance = entities.get(\"STANCE\")\n",
    "    \n",
    "    # Logic to retrieve or generate argument from NLAS-multi corpus\n",
    "    # This is a placeholder logic\n",
    "    if topic:\n",
    "        return f\"Based on the topic {topic}, here's an argument from the corpus...\"\n",
    "    else:\n",
    "        return \"I need more information to generate an argument.\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "#### 4. Creating the User Interface\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>QA Chatbot</title>\n",
    "    <style>\n",
    "        /* Simple styles for the chatbot interface */\n",
    "        .chat-container {\n",
    "            width: 400px;\n",
    "            margin: auto;\n",
    "            background: #f7f7f7;\n",
    "            padding: 20px;\n",
    "            border-radius: 10px;\n",
    "        }\n",
    "        .chat-box {\n",
    "            width: 100%;\n",
    "            height: 300px;\n",
    "            border: 1px solid #ccc;\n",
    "            overflow-y: auto;\n",
    "            padding: 10px;\n",
    "        }\n",
    "        .input-box {\n",
    "            width: 100%;\n",
    "            padding: 10px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"chat-container\">\n",
    "        <div class=\"chat-box\" id=\"chat-box\"></div>\n",
    "        <input type=\"text\" id=\"user-input\" class=\"input-box\" placeholder=\"Ask a question...\">\n",
    "        <button onclick=\"sendQuestion()\">Send</button>\n",
    "    </div>\n",
    "    \n",
    "    <script>\n",
    "        function sendQuestion() {\n",
    "            const userInput = document.getElementById('user-input').value;\n",
    "            fetch('/get_argument', {\n",
    "                method: 'POST',\n",
    "                headers: {\n",
    "                    'Content-Type': 'application/json'\n",
    "                },\n",
    "                body: JSON.stringify({ question: userInput })\n",
    "            })\n",
    "            .then(response => response.json())\n",
    "            .then(data => {\n",
    "                const chatBox = document.getElementById('chat-box');\n",
    "                chatBox.innerHTML += `<p>User: ${userInput}</p>`;\n",
    "                chatBox.innerHTML += `<p>Bot: ${data.argument}</p>`;\n",
    "                document.getElementById('user-input').value = '';\n",
    "                chatBox.scrollTop = chatBox.scrollHeight;\n",
    "            });\n",
    "        }\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "### Testing and Deployment\n",
    "\n",
    "- **Testing:** Ensure the chatbot handles various types of questions and correctly identifies entities.\n",
    "- **Deployment:** Deploy the Flask app to a web server and make the interface accessible online.\n",
    "\n",
    "By following these steps, you will have a functional QA chatbot that integrates with the NLAS-multi corpus to provide arguments based on user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505caa8-0236-432f-8a5e-aa3658141cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Creating a Question Answering (QA) chatbot that integrates the NLAS-multi corpus for generating natural language arguments \n",
    "involves several steps, including training a Named Entity Recognition (NER) model, developing the chatbot backend, and \n",
    "creating a user-friendly interface. Below, I'll outline the key components and provide example code snippets for each part \n",
    "of the implementation.\n",
    "\n",
    "### 1. Train a Named Entity Recognition (NER) Model\n",
    "First, we'll train an NER model to identify entities in user questions. For simplicity, we can use the spaCy library, which is widely \n",
    "used for NER tasks.\n",
    "\n",
    "#### Step 1.1: Install and Load spaCy\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "#### Step 1.2: Train the NER Model\n",
    "You can customize and train a new NER model with spaCy if needed, but for this example, we'll use the pre-trained model.\n",
    "\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example function to extract named entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Test the NER extraction\n",
    "text = \"Is there an expert positioned in favor of climate change policies?\"\n",
    "entities = extract_entities(text)\n",
    "print(entities)\n",
    "```\n",
    "\n",
    "### 2. Integrate the NLAS-Multi Corpus\n",
    "The NLAS-multi corpus can be downloaded and integrated. For this example, we'll assume the corpus is a collection of argument \n",
    "texts that can be searched or indexed for relevant arguments.\n",
    "\n",
    "#### Step 2.1: Load the Corpus\n",
    "Download and preprocess the corpus to make it searchable.\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Load the NLAS-multi corpus\n",
    "with open(\"path_to_NLAS_multi_corpus.json\", \"r\") as file:\n",
    "    corpus = json.load(file)\n",
    "\n",
    "# Example structure of the corpus (assuming JSON format)\n",
    "# corpus = [\n",
    "#     {\"topic\": \"climate change\", \"stance\": \"in favor\", \"argument\": \"...\"},\n",
    "#     {\"topic\": \"climate change\", \"stance\": \"against\", \"argument\": \"...\"},\n",
    "#     ...\n",
    "# ]\n",
    "\n",
    "def find_argument(topic, stance):\n",
    "    for entry in corpus:\n",
    "        if entry['topic'].lower() == topic.lower() and entry['stance'].lower() == stance.lower():\n",
    "            return entry['argument']\n",
    "    return \"No argument found for the given topic and stance.\"\n",
    "\n",
    "# Example usage\n",
    "topic = \"climate change\"\n",
    "stance = \"in favor\"\n",
    "argument = find_argument(topic, stance)\n",
    "print(argument)\n",
    "\n",
    "\n",
    "### 3. Develop the Chatbot Backend\n",
    "Create a chatbot backend to process user inputs, perform NER, and retrieve arguments from the corpus.\n",
    "\n",
    "class QAChatbot:\n",
    "    def __init__(self, nlp, corpus):\n",
    "        self.nlp = nlp\n",
    "        self.corpus = corpus\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        entities = {ent.label_: ent.text for ent in doc.ents}\n",
    "        return entities\n",
    "    \n",
    "    def find_argument(self, topic, stance):\n",
    "        for entry in self.corpus:\n",
    "            if entry['topic'].lower() == topic.lower() and entry['stance'].lower() == stance.lower():\n",
    "                return entry['argument']\n",
    "        return \"No argument found for the given topic and stance.\"\n",
    "    \n",
    "    def respond(self, question):\n",
    "        entities = self.extract_entities(question)\n",
    "        topic = entities.get('TOPIC')\n",
    "        stance = entities.get('STANCE')\n",
    "        if topic and stance:\n",
    "            return self.find_argument(topic, stance)\n",
    "        else:\n",
    "            return \"Could not identify the topic or stance from the question.\"\n",
    "\n",
    "# Initialize the chatbot\n",
    "chatbot = QAChatbot(nlp, corpus)\n",
    "\n",
    "# Example question\n",
    "question = \"Is there an expert positioned in favor of climate change policies?\"\n",
    "response = chatbot.respond(question)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "### 4. Develop a User-Friendly Interface\n",
    "Using a web framework like Flask, we can create a simple web interface for the chatbot.\n",
    "\n",
    "#### Step 4.1: Install Flask\n",
    "```bash\n",
    "pip install flask\n",
    "```\n",
    "\n",
    "#### Step 4.2: Create a Flask App\n",
    "```python\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "chatbot = QAChatbot(nlp, corpus)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/ask\", methods=[\"POST\"])\n",
    "def ask():\n",
    "    data = request.json\n",
    "    question = data.get(\"question\")\n",
    "    response = chatbot.respond(question)\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n",
    "```\n",
    "\n",
    "#### Step 4.3: Create HTML Template\n",
    "Create an `index.html` file in the `templates` directory with a simple form for user input.\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>QA Chatbot</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Ask the QA Chatbot</h1>\n",
    "    <form id=\"question-form\">\n",
    "        <input type=\"text\" id=\"question\" name=\"question\" placeholder=\"Enter your question\">\n",
    "        <button type=\"submit\">Ask</button>\n",
    "    </form>\n",
    "    <div id=\"response\"></div>\n",
    "\n",
    "    <script>\n",
    "        document.getElementById(\"question-form\").addEventListener(\"submit\", function(event) {\n",
    "            event.preventDefault();\n",
    "            const question = document.getElementById(\"question\").value;\n",
    "            fetch(\"/ask\", {\n",
    "                method: \"POST\",\n",
    "                headers: {\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                body: JSON.stringify({ question: question })\n",
    "            })\n",
    "            .then(response => response.json())\n",
    "            .then(data => {\n",
    "                document.getElementById(\"response\").innerText = data.response;\n",
    "            });\n",
    "        });\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "This example provides a basic implementation of a QA chatbot that uses NER for question pre-processing and retrieves arguments from the NLAS-multi corpus. The Flask web framework is used to create a simple user interface for interacting with the chatbot. You can expand and customize this implementation based on specific requirements and the actual structure of the NLAS-multi corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb478530-e31e-409f-9324-a6a65bdc50cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 79) (3956786512.py, line 79)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 79\u001b[1;36m\u001b[0m\n\u001b[1;33m    We'll build and train a Bi-LSTM model using TensorFlow/Keras.\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 79)\n"
     ]
    }
   ],
   "source": [
    "# To use the NLAS-multi corpus for training an NER model, we'll need to follow these steps:\n",
    "\n",
    "# 1. **Download and Preprocess the Data**: Load the NLAS-multi corpus and preprocess it to a suitable format for training.\n",
    "# 2. **Feature Extraction**: Convert the text and labels into numerical format that can be fed into a machine learning model.\n",
    "# 3. **Model Building and Training**: Use a machine learning framework (e.g., TensorFlow/Keras) to build and train the NER model.\n",
    "# 4. **Model Evaluation and Inference**: Evaluate the trained model and demonstrate how to use it for NER tasks.\n",
    "\n",
    "### 1. Download and Preprocess the Data\n",
    "\n",
    "# First, download the NLAS-multi corpus from the provided link and load it into a Python environment.\n",
    "\n",
    "#### Step 1.1: Download the Data\n",
    "# ```bash\n",
    "wget https://zenodo.org/record/8364002/files/nlas-multi.zip?download=1 -O nlas-multi.zip\n",
    "unzip nlas-multi.zip -d nlas-multi\n",
    "# ```\n",
    "\n",
    "# Step 1.2: Load and Preprocess the Data\n",
    "\n",
    "# Assuming the NLAS-multi corpus is in a JSON format with sentences and their corresponding tags, we'll load and preprocess the data. Here's an example of how to do that:\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the NLAS-multi corpus\n",
    "with open('nlas-multi/nlas-multi.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract sentences and tags\n",
    "sentences = [entry['sentence'] for entry in data]\n",
    "tags = [entry['tags'] for entry in data]\n",
    "\n",
    "# Build vocabulary and tag indices\n",
    "words = set(word for sentence in sentences for word in sentence)\n",
    "tags_set = set(tag for tag_seq in tags for tag in tag_seq)\n",
    "\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"PAD\"] = 0\n",
    "word2idx[\"UNK\"] = 1\n",
    "\n",
    "tag2idx = {t: i for i, t in enumerate(tags_set)}\n",
    "tag2idx[\"PAD\"] = len(tag2idx)\n",
    "\n",
    "# Convert sentences and tags to sequences of indices\n",
    "X = [[word2idx.get(w, word2idx[\"UNK\"]) for w in sentence] for sentence in sentences]\n",
    "y = [[tag2idx[t] for t in tag_seq] for tag_seq in tags]\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(len(s) for s in sentences)\n",
    "X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "y = pad_sequences(y, padding='post', maxlen=max_len)\n",
    "\n",
    "# Convert tags to categorical (one-hot encoding)\n",
    "y = [to_categorical(i, num_classes=len(tag2idx)) for i in y]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "```\n",
    "\n",
    "### 2. Model Building and Training\n",
    "\n",
    "# We'll build and train a Bi-LSTM model using TensorFlow/Keras.\n",
    "\n",
    "#### Step 2.1: Build the Model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, InputLayer\n",
    "\n",
    "# Parameters\n",
    "input_dim = len(word2idx)\n",
    "output_dim = 50  # Embedding output dimension\n",
    "input_length = max_len  # Input sequence length\n",
    "n_tags = len(tag2idx)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(input_length,)))\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)))\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "#### Step 2.2: Train the Model\n",
    "```python\n",
    "history = model.fit(X_train, np.array(y_train), validation_split=0.1, batch_size=32, epochs=5, verbose=1)\n",
    "```\n",
    "\n",
    "### 3. Model Evaluation\n",
    "\n",
    "#### Step 3.1: Evaluate the Model\n",
    "```python\n",
    "loss, accuracy = model.evaluate(X_test, np.array(y_test))\n",
    "print(f\"Loss: {loss}, Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "### 4. Inference\n",
    "\n",
    "#### Step 4.1: Predict Tags for a Sentence\n",
    "```python\n",
    "# Create a reverse dictionary to map indices back to tags\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "# Function to decode predictions\n",
    "def predict(sentence):\n",
    "    # Convert sentence to indices\n",
    "    sentence_idx = [word2idx.get(w, word2idx[\"UNK\"]) for w in sentence]\n",
    "    sentence_idx = pad_sequences([sentence_idx], maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(sentence_idx)\n",
    "    pred_tags = [idx2tag[np.argmax(tag)] for tag in pred[0]]\n",
    "    \n",
    "    return list(zip(sentence, pred_tags))\n",
    "\n",
    "# Example usage\n",
    "sentence = [\"Apple\", \"is\", \"looking\", \"at\", \"buying\", \"U.K.\", \"startup\", \"for\", \"$1\", \"billion\", \".\"]\n",
    "predicted_tags = predict(sentence)\n",
    "print(predicted_tags)\n",
    "```\n",
    "\n",
    "### Summary\n",
    "\n",
    "# This implementation provides a complete example of training an NER model from scratch using the NLAS-multi corpus and TensorFlow/Keras. The steps include downloading and preprocessing the data, building and training the model, evaluating the model, and using the model for inference. You can further refine and optimize this pipeline based on your specific needs and the structure of the NLAS-multi corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
